{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch import nn\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from CLAHE paper and cv2 documentation\n",
    "class CLAHETransform:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_np = np.array(img)\n",
    "        # Check if image is not grayscale, convert it\n",
    "        if len(img_np.shape) == 3:\n",
    "            img_gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            img_gray = img_np\n",
    "        # Apply CLAHE\n",
    "        clahe = cv2.createCLAHE(clipLimit=self.clip_limit, tileGridSize=self.tile_grid_size)\n",
    "        img_clahe = clahe.apply(img_gray)\n",
    "        # If original image was RGB, replace the luminance component with the processed one\n",
    "        if len(img_np.shape) == 3:\n",
    "            img_clahe = cv2.cvtColor(img_np, cv2.COLOR_RGB2YCrCb)\n",
    "            img_clahe[:,:,0] = clahe.apply(img_clahe[:,:,0])\n",
    "            img_clahe = cv2.cvtColor(img_clahe, cv2.COLOR_YCrCb2RGB)\n",
    "        # Convert numpy array back to PIL image\n",
    "        img = Image.fromarray(img_clahe)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Loop through the files in red folder and combine, into a dictionary, the other bands\n",
    "        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in r_dir.iterdir() if not f.is_dir()]\n",
    "        self.pytorch = pytorch\n",
    "        self.transform = transform\n",
    "        \n",
    "    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n",
    "        \n",
    "        files = {'red': r_file, \n",
    "                 'green':g_dir/r_file.name.replace('red', 'green'),\n",
    "                 'blue': b_dir/r_file.name.replace('red', 'blue'), \n",
    "                 'nir': nir_dir/r_file.name.replace('red', 'nir'),\n",
    "                 'gt': gt_dir/r_file.name.replace('red', 'gt')}\n",
    "\n",
    "        return files\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files)\n",
    "     \n",
    "    def open_as_array(self, idx, invert=False, include_nir=False):\n",
    "\n",
    "        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n",
    "                            np.array(Image.open(self.files[idx]['green'])),\n",
    "                            np.array(Image.open(self.files[idx]['blue'])),\n",
    "                           ], axis=2)\n",
    "    \n",
    "        if include_nir:\n",
    "           img_nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n",
    "           raw_rgb = np.concatenate([raw_rgb, img_nir], axis=2)\n",
    "    \n",
    "        if invert:\n",
    "            raw_rgb = raw_rgb.transpose((2,0,1))\n",
    "    \n",
    "        # normalize\n",
    "        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n",
    "    \n",
    "\n",
    "    def open_mask(self, idx, add_dims=False):\n",
    "        \n",
    "        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n",
    "        raw_mask = np.where(raw_mask==255, 1, 0)\n",
    "        \n",
    "        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=False), dtype=torch.float32)\n",
    "        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n",
    "        \n",
    "        if self.transform: \n",
    "            x = self.transform(x)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def open_as_pil(self, idx):\n",
    "        \n",
    "        arr = 256*self.open_as_array(idx)\n",
    "        \n",
    "        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = 'Dataset class with {} files'.format(self.__len__())\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply image augnmentation for the dataset\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    # CLAHETransform(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path(\"95-Cloud/95-cloud_training_only_additional_to38-cloud\")\n",
    "dataset = CloudDataset(base_path/'train_red_additional_to38cloud', \n",
    "                    base_path/'train_green_additional_to38cloud', \n",
    "                    base_path/'train_blue_additional_to38cloud', \n",
    "                    base_path/'train_nir_additional_to38cloud',\n",
    "                    base_path/'train_gt_additional_to38cloud',\n",
    "                    transform=data_transform)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test x, y shape\n",
    "x,y=dataset[1]\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_size = int(0.8 * len(dataset))\n",
    "train_size = int(0.8 * train_valid_size)\n",
    "valid_size = train_valid_size - train_size\n",
    "test_size = len(dataset) - train_valid_size\n",
    "train_ds, valid_ds, test_ds = torch.utils.data.random_split(dataset, (train_size, valid_size, test_size))\n",
    "print(len(train_ds), len(valid_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=16, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "        \n",
    "\n",
    "\n",
    "class RCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurr_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=2,t=2):\n",
    "        super(recurr_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.RCNN1 = RCNN_block(ch_in=img_ch,ch_out=32,t=t)\n",
    "        self.RCNN2 = RCNN_block(ch_in=32,ch_out=64,t=t)\n",
    "        self.RCNN3 = RCNN_block(ch_in=64,ch_out=128,t=t)\n",
    "        self.RCNN4 = RCNN_block(ch_in=128,ch_out=256,t=t)\n",
    "\n",
    "        self.upconv4 = self.expand_block(256, 128, 3, 1)\n",
    "        self.upconv3 = self.expand_block(128*2, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, output_ch, 3, 1)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # # encoding path\n",
    "        x1 = self.RCNN1(x)\n",
    "        x1 = self.Maxpool(x1)\n",
    "\n",
    "        x2 = self.RCNN2(x1)\n",
    "        x2 = self.Maxpool(x2)\n",
    "\n",
    "        x3 = self.RCNN3(x2)\n",
    "        x3 = self.Maxpool(x3)\n",
    "\n",
    "        x4 = self.RCNN4(x3)\n",
    "        x4 = self.Maxpool(x4)\n",
    "\n",
    "        upconv4 = self.upconv4(x4)\n",
    "        upconv3 = self.upconv3(torch.cat([upconv4, x3], 1))\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, x2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, x1], 1))\n",
    "\n",
    "        return upconv1\n",
    "    \n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = recurr_Net()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    unet = nn.DataParallel(unet) \n",
    "    unet.to(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "count_parameters(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "    \n",
    "#     scheduler = StepLR(opt, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss.detach()*dataloader.batch_size\n",
    "                if step % 100 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            clear_output(wait=True)\n",
    "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "            print('-' * 10)\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "            print('-' * 10)\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "#         scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    \n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "train_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs=75)\n",
    "\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime(\"%m_%d_%Y_%H%M%S\")\n",
    "path = \"R2_unet_\" + formatted_time + \"_acc_\" +\".pth\"\n",
    "torch.save(unet, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = recurr_Net()\n",
    "device = 'cuda:0'\n",
    "# Load the pre-trained weights\n",
    "checkpoint = torch.load(\"recurrent_unet_12_17_2023_235302_wo_CLAHE.pth\")\n",
    "unet = checkpoint.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, jaccard_score\n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()\n",
    "\n",
    "# Initialize accumulators\n",
    "test_acc, test_precision, test_recall, test_f1, test_auc, test_jaccard = 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "all_probs, all_targets = [], []\n",
    "# Iterate over the test dataset\n",
    "for xt, yt in test_dl:\n",
    "    with torch.no_grad():\n",
    "        predt = unet(xt.cuda())\n",
    "    # Compute accuracy\n",
    "    acc = acc_metric(predt, yt)\n",
    "    test_acc += acc.item() * xt.size(0)\n",
    "    # Reshape and process predictions and labels\n",
    "    preds = predt.argmax(dim=1).view(-1).cpu().numpy()  # Convert to binary predictions and flatten\n",
    "    labels = yt.view(-1).cpu().numpy()  # Flatten the ground truth labels\n",
    "    # Accumulate all probabilities (for class 1) and true labels for AUC calculation\n",
    "    probs = predt[:, 1, :, :].sigmoid().view(-1).cpu().numpy()\n",
    "    all_probs.extend(probs)\n",
    "    all_targets.extend(labels)\n",
    "    # Calculate and accumulate precision, recall, F1, and Jaccard for each batch\n",
    "    test_precision += precision_score(labels, preds) * xt.size(0)\n",
    "    test_recall += recall_score(labels, preds) * xt.size(0)\n",
    "    test_f1 += f1_score(labels, preds) * xt.size(0)\n",
    "    test_jaccard += jaccard_score(labels, preds) * xt.size(0)\n",
    "# Calculate averages over the entire test set\n",
    "num_samples = len(test_dl.dataset)\n",
    "test_acc = test_acc / num_samples\n",
    "test_precision = test_precision / num_samples\n",
    "test_recall = test_recall / num_samples\n",
    "test_f1 = test_f1 / num_samples\n",
    "test_jaccard = test_jaccard / num_samples\n",
    "test_auc = roc_auc_score(all_targets, all_probs)\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Average Jaccard (IoU): {test_jaccard:.4f}\")\n",
    "print(f\"AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert tensor list to list\n",
    "for i in range(len(train_loss)):\n",
    "    train_loss[i] = float(train_loss[i].item())\n",
    "    valid_loss[i] = float(valid_loss[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(train_loss, label='Train loss')\n",
    "plt.plot(valid_loss, label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = next(iter(test_dl))\n",
    "\n",
    "with torch.no_grad():\n",
    "    predb = unet(xb.cuda())\n",
    "\n",
    "predb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "formatted_time = now.strftime(\"%m_%d_%Y_%H%M%S\")\n",
    "path = \"models/recurrent_unet_\" + formatted_time + \"_acc_\" + str(round(test_acc.item(), 4)) +\".pth\"\n",
    "torch.save(unet, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "state_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
